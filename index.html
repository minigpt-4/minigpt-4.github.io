<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Minigpt-4">
  <meta name="keywords" content="GPT-4, open-source, vision-language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Minigpt-4</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MiniGPT-4:</h1>
          <h2 class="title is-2 publication-title">Generalized Decoding for Pixel, Image and Language</h2>
          <div class="is-size-5">
            <span class="author-block">
                <a href="https://maureenzou.github.io/" style="color:#f68946;font-weight:normal;">Deyao Zhu<sup>*</sup>
                </a>,                
            </span>
            <span class="author-block">
              <a href="https://zdou0830.github.io/" style="color:#F2A900;font-weight:normal;">Zi-Yi Dou<sup>*</sup></a>,</span>
            <span class="author-block">
              <a href="https://jwyang.github.io/" style="color:#00A4EF;font-weight:normal;">Jianwei Yang<sup>*&#x2691;</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://zhegan27.github.io/" style="color:#008AD7;font-weight:normal;">Zhe Gan</a>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/linjli/" style="color:#008AD7;font-weight:normal;">Linjie Li</a>,
            </span>
            <span class="author-block">
              <a href="https://chunyuan.li/" style="color:#00A4EF;font-weight:normal;">Chunyuan Li</a>,
            </span>  
            <span class="author-block">
              <a href="https://sites.google.com/site/xiyangdai/" style="color:#008AD7;font-weight:normal;">Xiyang Dai</a>,
            </span>   
            <span class="author-block">
              <a href="https://harkiratbehl.github.io/" style="color:#00A4EF;font-weight:normal;">Harkirat Behl</a>,
            </span>                                  
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en" style="color:#008AD7;font-weight:normal;">Jianfeng Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/luyuan/" style="color:#008AD7;font-weight:normal;">Lu Yuan</a>,
            </span>       
            <span class="author-block">
              <a href="https://vnpeng.net/" style="color:#F2A900;font-weight:normal;">Nanyun Peng</a>,
            </span>       
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/lijuanw/" style="color:#008AD7;font-weight:normal;">Lijuan Wang</a>,
            </span>    
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae Lee<sup>&#x2628;</sup></a>,
            </span>                                                
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/" style="color:#00A4EF;font-weight:normal;">Jianfeng Gao<sup>&#x2628;</sup></a>
            </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of Wisconsin-Madison; </b></span>
            <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>UCLA; </span>
            <span class="author-block"><b style="color:#00A4EF; font-weight:normal">&#x25B6 </b>Microsoft Research, Redmond; </span>
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>Microsoft Cloud & AI </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Technical Contribution, </span>
            <span class="author-block"><sup>&#x2628;</sup>Equal Advisory Contribution, </span>
            <span class="author-block"><sup>&#x2691;</sup>Project Lead </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#e08ba0; font-weight:normal"> <b>In CVPR2023</b> </b></span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2212.11270.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/microsoft/X-Decoder/tree/main" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/microsoft/X-Decoder/tree/xgpt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-hotjar"></i>
                  </span>
                  <span>X-GPT</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/spaces/xdecoder/Demo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-laugh-beam"></i>
                  </span>
                  <span>All-in-One Demo</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/spaces/xdecoder/Instruct-X-Decoder"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-laugh-beam"></i>
                  </span>
                  <span>Instruct Demo</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://eval.ai/web/challenges/challenge-page/1931/overview"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-tree"></i>
                  </span>
                  <span>Challenge</span>
                  </a>
              </span>
              
              <span class="link-block">
                <a href="https://youtu.be/wYp6vmyolqE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="120%" src="images/tittle_fig.gif">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. X-Decoder is a single model trained to support a wide range of vision and vision-language tasks.</b></p>
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present X-Decoder, a generalized decoding pipeline that can predict pixel-level segmentation and language tokens seamlessly. 
            X-Decoder takes as inputs two types of queries: (i) generic non-semantic queries and 
            (ii) semantic queries induced from text inputs, to decode different pixel-level and token-level outputs in the same semantic space. With such a novel design, 
            <b>X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks</b>. 
            After pretraining on a mixed set of a limited amount of segmentation data and million of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings. 
            Notably, it achieves <b>(1) state-of-the-art open-vocabulary segmentation and referring
            segmentation on 10 settings of 7 datasets; (2) better or competitive finetuned performance to other generalist and specialist models on segmentation and VL tasks; 
            and (3) flexibility for efficient finetuning and novel task composition (e.g., referring captioning = referring segmentation + captioning).</b>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <br>
    <br>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://user-images.githubusercontent.com/11957155/209045241-916ccf73-d29d-4637-8502-027d3420875c.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

    <!--/ Demo. -->
    <br>
    <br>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo</h2>
      </div>
    </div>

    <div class="column is-full-width">
      <div class="columns is-centered">
        <img id="teaser" width="90%" src="images/demo6_AdobeExpress.gif">
      </div>
      <div class="columns is-centered">
      <h1>
        <p style="font-family:Times New Roman"><b>X-GPT: Connecting generalist X-Decoder with GPT-3</b>
      </h1>                 
      </div>
    </div>

    <br>

    <div class="column is-full-width">
      <div class="columns is-centered">
        <img id="teaser" width="90%" src="images/inpaint.gif">
      </div>
      <div class="columns is-centered">
      <h1>
        <p style="font-family:Times New Roman"><b>Instruct-X-Decoder: Object-centric instructional image editing</b>
      </h1>                 
      </div>
    </div>

    <!--/ Paper video. -->
    <br>
    <br>
    <!-- Paper Model. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Model</h2>
        <div class="content has-text-justified">
          <p>
            <b>Our X-Decoder is unique for three critical designs</b>:
          </p>
          <ul>
            <li>It has two types of queries (latent queries and text queries) and outputs (semantic outputs and pixel-level outputs).</li>
            <li>It uses a single text encoder for all text corpus, ranging from class concepts, referring phrases to image captions.</li>
            <li>It decouples image and text encoder to accomadate cross-image tasks (e.g., image-text retrieval) and within-image tasks (e.g., segmentation and captioning).</li>
          </ul>
        </div>        
        <img id="model" width="50%" src="images/method1.gif">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Figure 2. X-Decoder overall architecture.</b></p>
        </h3>   
        <br>
        <br>
        <div class="content has-text-justified">
          <p>
            <b>X-Decoder can be used to unify a variety of vision and vision-language tasks</b>:
          </p>
          <ul>
            <li><b>Generic Segmentation</b>: Instance, semantic and panoptic segmentation, all supporting open-vocabulary and zero-shot.</li>
            <li><b>Referring Segmentation</b>: Refer to specific image segment given arbitary textual queries from text encoder. </li>
            <li><b>Image-Text Retrieval</b>: Decoupled image and text representation extraction and dot-product for computing the similarities. </li>
            <li><b>Image Captioning</b>: Decode textual tokens using exactly the same decoder in autoregressive manner.</li>
          </ul>                    
        </div>          
        <img id="model" width="100%" src="images/method_2.png">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Figure 3. Unify different types of vision and vision-language tasks with a single X-Decoder.</b></p>
        </h3>        
        </div>
      </div>
    </div>
    <br>
    <br>    
    <!--/ Paper video. -->          
  </div>
</section>

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Results</h2>
      </div>
    </div>
  </div>
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <!-- Generic Segmentation. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Zero-Shot Segmentation</h2>
      <img id="teaser" width="120%" src="images/bar_chart.png">
      <h1>
        <p style="font-family:Times New Roman"><b>Chart 1. Left: Zero-shot Segmentation Result Compared with SoTA; Right: Finetuned Segmentation Result Compared with SoTA.</b>
      </h1>                 
    </div>
</div>

<div class="container is-max-desktop">

<div class="column is-full-width">
  <h2 class="title is-4">Segmentation In the Wild</h2>
  <img id="teaser" width="120%" src="images/zero-shot-seginw.png">
  <h1>
    <p style="font-family:Times New Roman"><b>Chart 2. Zero-shot instance segmentation performance on SeginW benchmark.</b>
  </h1>                 
</div>
</div>

<!-- <section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Demo</h2>
      </div>
    </div>
  </div>
<div class="container is-max-desktop">

  <h2 class="title is-4">Rerferring Image Editing and Inpainting</h2>
  <div class="column is-full-width">
    <div class="columns is-centered">
      <img id="teaser" width="100%" src="images/inpaint.gif">
    </div>
    <div class="columns is-centered">
    <h1>
      <p style="font-family:Times New Roman"><b>Demo. Left: Green Apple -> Rabbit, Darkest Red Apple -> Red Pear, Table -> Plate. Middle: Object/Stuff Removal, Right: Sky -> X</b>
    </h1>                 
  </div>

  </div> -->

<!-- <div class="column is-full-width">
  <h1><b>Referring Imaeg Inpainting</b></h1>
  <img id="teaser" width="100%" src="images/remove-dog.gif">
  <h1>
    <p style="font-family:Times New Roman"><b>Demo 2. Remove the dog.</b>
  </h1>                 
</div> -->
</div>

<section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Visualizations</h2>      
        </div>
      </div>
    </div>
    <!--/ Results. -->    
  <div class="container is-max-desktop">

    <!-- Generic Segmentation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Zero-Shot Segmentation</h2>
        <!-- <h1><b>Zero-Shot Segmentation</b></h1> -->
        <img id="teaser" width="120%" src="images/zero-shot-generic-image-segmentation.png">
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 4. Zero-shot semantic segmentation with pretrained X-Decoder on 10 settings of 7 datasets.</b>
        </h1>                       
      </div>
    </div>
    <!--/ Generic Segmentation. -->

    <!-- Video Segmentation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Zero-shot Segmentation for Videos</h2>
        <!-- <h1><b>Zero-shot Segmentation for Videos</b></h1> -->
        <img id="teaser" width="120%" src="images/zs_video_seg.png">
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 5. Zero-shot panoptic segmentation for <a href="https://youtube-vos.org/">YouTubeVOS</a> video dataset.</b>
        </h1>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Referring Segmentation for Videos</h2>
        <!-- <h1><b>Referring Segmentation for Videos</b></h1> -->
        <img id="teaser" width="120%" src="images/referring-segmentation.png">
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 6. Zero-shot referring segmentation for <a href="https://youtube-vos.org/">YouTubeVOS</a> video dataset.</b>
        </h1>
      </div>
    </div>


    <!-- VL Tasks. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Image Captioning</h2>
        <!-- <h1><b>Image Captioning</b></h1> -->
        <img id="teaser" width="120%" src="images/zero-shot-image-captioning-video-images.png">
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 7. Zero-shot image captioning on YoutubeVOS video frames.</b>
        </h1>
      </div>
    </div>
    <!--/ VL Tasks. -->

    
    <!-- Region Captioning. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Referring Image Captioning</h2>
        <!-- <h1><b>Referring Image Captioning</b></h1> -->
        <img id="teaser" width="120%" src="images/referring-captioning.png">
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 8. Zero-shot referring image captioning on COCO 2017 val images (pink regions are referred).</b>
        </h1>
      </div>
    </div>
    <!--/ Region Captioning. -->

    <!-- Region Editing. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Referring Image Editing</h2>
        <!-- <h1><b>Referring Image Editing</b></h1> -->
        <img id="teaser" width="120%" src="images/referring-image-editing.png">
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 9. Zero-shot referring image editing by combining X-Decoder with Stable Diffusion inpainting.</b>
        </h1>
      </div>
    </div>
    <!--/ Region Editing. -->    


    <!-- Segmentation in the Wild. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Segmentation in the Wild</h2>
          <img id="teaser" width="100%" src="images/seginw_examples.png">          
        <h1>
          <p style="font-family:Times New Roman"><b>Figure 10. Ground Truth visualization of segmentation in the wild datasets from <a href="https://universe.roboflow.com/">Roboflow</a> for a wider evaluation. </b>
        </h1>
      </div>
    </div>
    <!--/ Segmentation in the Wild. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>

@article{zou2022xdecoder,
  author      = {Zou*, Xueyan and Dou*, Zi-Yi and Yang*, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee^, Yong Jae and Gao^, Jianfeng},
  title       = {Generalized Decoding for Pixel, Image and Language},
  publisher   = {arXiv:2212.11270v1},
  year        = {2022},
}
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


</body>
</html>
